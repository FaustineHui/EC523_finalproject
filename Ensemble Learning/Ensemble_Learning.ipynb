{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1842f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides!=1:\n",
    "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip=None\n",
    "\n",
    "        rep=[]\n",
    "\n",
    "        filters=in_filters\n",
    "        if grow_first:\n",
    "            rep.append(nn.ReLU(inplace=True))\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(nn.ReLU(inplace=True))\n",
    "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(nn.ReLU(inplace=True))\n",
    "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3,strides,1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x+=skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        #do relu here\n",
    "\n",
    "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
    "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
    "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
    "\n",
    "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        #do relu here\n",
    "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        # #------- init weights --------\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        #     elif isinstance(m, nn.BatchNorm2d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        # #-----------------------------\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = nn.ReLU(inplace=True)(features)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def xception(num_classes=2):\n",
    "    model = Xception(num_classes=num_classes)\n",
    "    model.last_linear = model.fc\n",
    "    del model.fc\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621ca233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNetBottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(DenseNetBottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(DenseNetBottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da07646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        self.inplanes = 64\n",
    "        # Special attributs\n",
    "        self.input_space = None\n",
    "        self.input_size = (299, 299, 3)\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        super(ResNet, self).__init__()\n",
    "        # Modules\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.conv1(input)\n",
    "        self.conv1_input = x.clone()\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.avgpool(features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=2)\n",
    "    # model.load_state_dict(torch.load(path_to_pretrained))\n",
    "    return model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833c7031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xception_car = xception(num_classes=2)\n",
    "Xception_car = nn.DataParallel(Xception_car)\n",
    "Xception_car.load_state_dict(torch.load('Xception_car_best.pt'))\n",
    "\n",
    "Xception_cat = xception(num_classes=2)\n",
    "Xception_cat = nn.DataParallel(Xception_cat)\n",
    "Xception_cat.load_state_dict(torch.load('Xception_cat_best.pt'))\n",
    "\n",
    "Xception_horse = xception(num_classes=2)\n",
    "Xception_horse = nn.DataParallel(Xception_horse)\n",
    "Xception_horse.load_state_dict(torch.load('Xception_horse_best.pt'))\n",
    "\n",
    "Xception_person = xception(num_classes=2)\n",
    "Xception_person = nn.DataParallel(Xception_person)\n",
    "Xception_person.load_state_dict(torch.load('Xception_person_best.pt'))\n",
    "\n",
    "Xception_sofa = xception(num_classes=2)\n",
    "Xception_sofa = nn.DataParallel(Xception_sofa)\n",
    "Xception_sofa.load_state_dict(torch.load('Xception_sofa_best.pt'))\n",
    "\n",
    "DenseNet_car = DenseNet(growthRate=24, depth=100, reduction=0.5,bottleneck=True, nClasses=2).cuda()\n",
    "DenseNet_car = nn.DataParallel(DenseNet_car)\n",
    "DenseNet_car.load_state_dict(torch.load('DenseNet_car_best.pt'))\n",
    "\n",
    "DenseNet_cat = DenseNet(growthRate=24, depth=100, reduction=0.5,bottleneck=True, nClasses=2).cuda()\n",
    "DenseNet_cat = nn.DataParallel(DenseNet_cat)\n",
    "DenseNet_cat.load_state_dict(torch.load('DenseNet_cat_best.pt'))\n",
    "\n",
    "DenseNet_horse = DenseNet(growthRate=24, depth=100, reduction=0.5,bottleneck=True, nClasses=2).cuda()\n",
    "DenseNet_horse = nn.DataParallel(DenseNet_horse)\n",
    "DenseNet_horse.load_state_dict(torch.load('DenseNet_horse_best.pt'))\n",
    "\n",
    "DenseNet_person = DenseNet(growthRate=24, depth=100, reduction=0.5,bottleneck=True, nClasses=2).cuda()\n",
    "DenseNet_person = nn.DataParallel(DenseNet_person)\n",
    "DenseNet_person.load_state_dict(torch.load('DenseNet_person_best.pt'))\n",
    "\n",
    "DenseNet_sofa = DenseNet(growthRate=24, depth=100, reduction=0.5,bottleneck=True, nClasses=2).cuda()\n",
    "DenseNet_sofa = nn.DataParallel(DenseNet_sofa)\n",
    "DenseNet_sofa.load_state_dict(torch.load('DenseNet_sofa_best.pt'))\n",
    "\n",
    "Spec_car = ResNet34()\n",
    "Spec_car = nn.DataParallel(Spec_car)\n",
    "Spec_car.load_state_dict(torch.load('Spec_car_best.pt'))\n",
    "\n",
    "Spec_cat = ResNet34()\n",
    "Spec_cat = nn.DataParallel(Spec_cat)\n",
    "Spec_cat.load_state_dict(torch.load('Spec_cat_best.pt'))\n",
    "\n",
    "Spec_horse = ResNet34()\n",
    "Spec_horse = nn.DataParallel(Spec_horse)\n",
    "Spec_horse.load_state_dict(torch.load('Spec_horse_best.pt'))\n",
    "\n",
    "Spec_person = ResNet34()\n",
    "Spec_person = nn.DataParallel(Spec_person)\n",
    "Spec_person.load_state_dict(torch.load('Spec_person_best.pt'))\n",
    "\n",
    "Spec_sofa = ResNet34()\n",
    "Spec_sofa = nn.DataParallel(Spec_sofa)\n",
    "Spec_sofa.load_state_dict(torch.load('Spec_sofa_best.pt'))\n",
    "\n",
    "ResNet34DCT_car = ResNet34()\n",
    "ResNet34DCT_car = nn.DataParallel(ResNet34DCT_car)\n",
    "ResNet34DCT_car.load_state_dict(torch.load('ResNet34DCT_car_best.pt'))\n",
    "\n",
    "ResNet34DCT_cat = ResNet34()\n",
    "ResNet34DCT_cat = nn.DataParallel(ResNet34DCT_cat)\n",
    "ResNet34DCT_cat.load_state_dict(torch.load('ResNet34DCT_cat_best.pt'))\n",
    "\n",
    "ResNet34DCT_horse = ResNet34()\n",
    "ResNet34DCT_horse = nn.DataParallel(ResNet34DCT_horse)\n",
    "ResNet34DCT_horse.load_state_dict(torch.load('ResNet34DCT_horse_best.pt'))\n",
    "\n",
    "ResNet34DCT_person = ResNet34()\n",
    "ResNet34DCT_person = nn.DataParallel(ResNet34DCT_person)\n",
    "ResNet34DCT_person.load_state_dict(torch.load('ResNet34DCT_person_best.pt'))\n",
    "\n",
    "ResNet34DCT_sofa = ResNet34()\n",
    "ResNet34DCT_sofa = nn.DataParallel(ResNet34DCT_sofa)\n",
    "ResNet34DCT_sofa.load_state_dict(torch.load('ResNet34DCT_sofa_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe79c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XceptionTestConstructor(path_to_test, classes, num_workers=4):\n",
    "    \n",
    "  \n",
    "    # Transformations to the image, edit as need be\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize([299,299]),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(path_to_test, transform=transform)\n",
    "#     print(\"Successfully Loaded Test Set.\")\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, \n",
    "        num_workers=num_workers)\n",
    "#     if classes != None:\n",
    "#         print(\"Number of Classes:\", len(classes))\n",
    "    return test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137f975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNetTestConstructor(path_to_test, classes=None, num_workers=4):\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize([32,32]),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(path_to_test, transform=test_transform)\n",
    "#     print(\"Successfully Loaded Test Set.\")\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, num_workers=num_workers)\n",
    "\n",
    "#     if classes != None:\n",
    "#         print(\"Number of Classes:\", len(classes))\n",
    "\n",
    "    return test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddec3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpectrumTestConstructor(path_to_test, classes=None, num_workers=4):\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(path_to_test, transform=test_transform)\n",
    "#     print(\"Successfully Loaded Test Set.\")\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, num_workers=num_workers)\n",
    "\n",
    "#     if classes != None:\n",
    "#         print(\"Number of Classes:\", len(classes))\n",
    "\n",
    "    return test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536adbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = nn.Softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3769643",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_categories = ['airplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']\n",
    "\n",
    "for k in range(20):\n",
    "    print('For test category ' + test_categories[k] + ':')\n",
    "\n",
    "    xcp_test_loader, image_classes = XceptionTestConstructor(path_to_test='F:/data/testset/progan/'+ test_categories[k],\n",
    "                                                               classes=['Real','Fake'],num_workers=1)\n",
    "    dsn_test_loader, image_classes = DenseNetTestConstructor(path_to_test='F:/data/testset/progan/'+ test_categories[k],\n",
    "                                                               classes=['Real','Fake'],num_workers=1)\n",
    "    fft_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/FFT/CNN_synth_testset/progan/'\n",
    "                                                             + test_categories[k],classes=['Real','Fake'],num_workers=1)\n",
    "    dct_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/CNN_synth_testset/progan/'\n",
    "                                                             + test_categories[k],classes=['Real','Fake'],num_workers=1)\n",
    "\n",
    "    car_model = []\n",
    "    test_loader_all = []\n",
    "    test_loader_all.append(xcp_test_loader)\n",
    "    test_loader_all.append(dsn_test_loader)\n",
    "    test_loader_all.append(fft_test_loader)\n",
    "    test_loader_all.append(dct_test_loader)\n",
    "    car_model.append(Xception_sofa)\n",
    "    car_model.append(DenseNet_sofa)\n",
    "    car_model.append(Spec_sofa)\n",
    "    car_model.append(ResNet34DCT_sofa)\n",
    "\n",
    "    weights_real = []\n",
    "    weights_fake = []\n",
    "    target_all = []\n",
    "    for j in range(4):\n",
    "        weight_real = []\n",
    "        weight_fake = []\n",
    "        target_single = []\n",
    "        model = car_model[j]\n",
    "        model.eval()\n",
    "        # specify loss function (categorical cross-entropy)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # specify optimizer\n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': list(model.parameters())[:-1], 'lr': 1e-3, 'momentum': 0.9, 'weight_decay': 1e-3},\n",
    "            {'params': list(model.parameters())[-1], 'lr': 5e-5, 'momentum': 0.9, 'weight_decay': 1e-5}\n",
    "        ])\n",
    "        # iterate over test data\n",
    "        for data, target in test_loader_all[j]:\n",
    "        #     print(target)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "        #     print(output)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # compare predictions to true label\n",
    "            probabilities = convert(output)\n",
    "            num_output = probabilities.cpu().detach().numpy()\n",
    "            num_target = target.cpu().detach().numpy()\n",
    "            for i in range(2):\n",
    "                weight_real.append(num_output[i][0])\n",
    "                weight_fake.append(num_output[i][1])\n",
    "                target_single.append(num_target[i])\n",
    "\n",
    "        weights_real.append(weight_real)\n",
    "        weights_fake.append(weight_fake)\n",
    "        target_all.append(target_single)\n",
    "    \n",
    "    count_real_correct = 0\n",
    "    count_fake_correct = 0\n",
    "    for i in range(400):\n",
    "        weighted_real = (weights_real[0][i] + weights_real[1][i] + weights_real[2][i] + weights_real[3][i])/4\n",
    "        weighted_fake = (weights_fake[0][i] + weights_fake[1][i] + weights_fake[2][i] + weights_fake[3][i])/4\n",
    "        if weighted_real > weighted_fake:\n",
    "            if i < 200:\n",
    "                count_real_correct += 1\n",
    "        else:\n",
    "            if i >= 200:\n",
    "                count_fake_correct += 1\n",
    "    \n",
    "    print('Test Accuracy of Real: %2d%% (%2d/%2d)'%(100*count_real_correct/200, count_real_correct, 200))\n",
    "    print('Test Accuracy of Fake: %2d%% (%2d/%2d)'%(100*count_fake_correct/200, count_fake_correct, 200))\n",
    "    print('Test Accuracy (Overall): %2d%% (%2d/%2d)'%(100*(count_real_correct+count_fake_correct)/400, \n",
    "                                                      count_real_correct+count_fake_correct, 400))\n",
    "    #     # average test loss)\n",
    "    #     test_loss = test_loss/len(test_loader.dataset)\n",
    "    #     print('Test Loss: {:.6f}'.format(test_loss))\n",
    "\n",
    "    #     for i in range(2):\n",
    "    #         if class_total[i] > 0:\n",
    "    #             print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "    #                 classes[i], 100 * class_correct[i] / class_total[i],\n",
    "    #                 np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    #         else:\n",
    "    #             print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    #     print('Test Accuracy (Overall): %2d%% (%2d/%2d)\\n' % (\n",
    "    #         100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    #         np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efc949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22710\\AppData\\Local\\Temp/ipykernel_7840/1079251366.py:98: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.fc(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Real: 81% (2200/2706)\n",
      "Test Accuracy of Fake:  5% (156/2698)\n",
      "Test Accuracy (Overall): 43% (2356/5404)\n"
     ]
    }
   ],
   "source": [
    "xcp_test_loader, image_classes = XceptionTestConstructor(path_to_test='F:/data/testset/deepfake',\n",
    "                                                           classes=['Real','Fake'],num_workers=1)\n",
    "dsn_test_loader, image_classes = DenseNetTestConstructor(path_to_test='F:/data/testset/deepfake',\n",
    "                                                           classes=['Real','Fake'],num_workers=1)\n",
    "fft_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/FFT/CNN_synth_testset/deepfake'\n",
    "                                                         ,classes=['Real','Fake'],num_workers=1)\n",
    "dct_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/CNN_synth_testset/deepfake'\n",
    "                                                         ,classes=['Real','Fake'],num_workers=1)\n",
    "\n",
    "car_model = []\n",
    "test_loader_all = []\n",
    "test_loader_all.append(xcp_test_loader)\n",
    "test_loader_all.append(dsn_test_loader)\n",
    "test_loader_all.append(fft_test_loader)\n",
    "test_loader_all.append(dct_test_loader)\n",
    "car_model.append(Xception_sofa)\n",
    "car_model.append(DenseNet_sofa)\n",
    "car_model.append(Spec_sofa)\n",
    "car_model.append(ResNet34DCT_sofa)\n",
    "\n",
    "weights_real = []\n",
    "weights_fake = []\n",
    "target_all = []\n",
    "for j in range(4):\n",
    "    weight_real = []\n",
    "    weight_fake = []\n",
    "    target_single = []\n",
    "    model = car_model[j]\n",
    "    model.eval()\n",
    "    # specify loss function (categorical cross-entropy)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # specify optimizer\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {'params': list(model.parameters())[:-1], 'lr': 1e-3, 'momentum': 0.9, 'weight_decay': 1e-3},\n",
    "        {'params': list(model.parameters())[-1], 'lr': 5e-5, 'momentum': 0.9, 'weight_decay': 1e-5}\n",
    "    ])\n",
    "    # iterate over test data\n",
    "    for data, target in test_loader_all[j]:\n",
    "    #     print(target)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "    #     print(output)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compare predictions to true label\n",
    "        probabilities = convert(output)\n",
    "        num_output = probabilities.cpu().detach().numpy()\n",
    "        num_target = target.cpu().detach().numpy()\n",
    "        for i in range(2):\n",
    "            weight_real.append(num_output[i][0])\n",
    "            weight_fake.append(num_output[i][1])\n",
    "            target_single.append(num_target[i])\n",
    "\n",
    "    weights_real.append(weight_real)\n",
    "    weights_fake.append(weight_fake)\n",
    "    target_all.append(target_single)\n",
    "\n",
    "count_real_correct = 0\n",
    "count_fake_correct = 0\n",
    "num_real = target_all[0].count(0)\n",
    "num_fake = target_all[0].count(1)\n",
    "for i in range(num_real+num_fake):\n",
    "    weighted_real = (weights_real[0][i] + weights_real[1][i] + weights_real[2][i] + weights_real[3][i])/4\n",
    "    weighted_fake = (weights_fake[0][i] + weights_fake[1][i] + weights_fake[2][i] + weights_fake[3][i])/4\n",
    "    if weighted_real > weighted_fake:\n",
    "        if i < num_real:\n",
    "            count_real_correct += 1\n",
    "    else:\n",
    "        if i >= num_real:\n",
    "            count_fake_correct += 1\n",
    "\n",
    "print('Test Accuracy of Real: %2d%% (%2d/%2d)'%(100*count_real_correct/num_real, count_real_correct, num_real))\n",
    "print('Test Accuracy of Fake: %2d%% (%2d/%2d)'%(100*count_fake_correct/num_fake, count_fake_correct, num_fake))\n",
    "print('Test Accuracy (Overall): %2d%% (%2d/%2d)'%(100*(count_real_correct+count_fake_correct)/(num_real+num_fake), \n",
    "                                                  count_real_correct+count_fake_correct, num_real+num_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d717cdbb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test category apple:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22710\\AppData\\Local\\Temp/ipykernel_8832/1079251366.py:98: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(self.fc(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Real: 82% (219/266)\n",
      "Test Accuracy of Fake: 11% (28/248)\n",
      "Test Accuracy (Overall): 48% (247/514)\n",
      "For test category horse:\n",
      "Test Accuracy of Real: 90% (109/120)\n",
      "Test Accuracy of Fake: 10% (14/140)\n",
      "Test Accuracy (Overall): 47% (123/260)\n",
      "For test category orange:\n",
      "Test Accuracy of Real: 76% (189/248)\n",
      "Test Accuracy of Fake:  1% ( 4/266)\n",
      "Test Accuracy (Overall): 37% (193/514)\n",
      "For test category summer:\n",
      "Test Accuracy of Real: 98% (302/308)\n",
      "Test Accuracy of Fake:  3% ( 8/238)\n",
      "Test Accuracy (Overall): 56% (310/546)\n",
      "For test category winter:\n",
      "Test Accuracy of Real: 99% (236/238)\n",
      "Test Accuracy of Fake:  7% (22/308)\n",
      "Test Accuracy (Overall): 47% (258/546)\n",
      "For test category zebra:\n",
      "Test Accuracy of Real: 87% (123/140)\n",
      "Test Accuracy of Fake: 20% (24/120)\n",
      "Test Accuracy (Overall): 56% (147/260)\n"
     ]
    }
   ],
   "source": [
    "test_categories = ['apple','horse','orange','summer','winter','zebra']\n",
    "# test_categories = ['bedroom','car','cat']\n",
    "# test_categories = ['car','cat','church','horse']\n",
    "# test_categories = ['apple']\n",
    "for k in range(6):\n",
    "    print('For test category ' + test_categories[k] + ':')\n",
    "\n",
    "    xcp_test_loader, image_classes = XceptionTestConstructor(path_to_test='F:/data/testset/cyclegan/'+ test_categories[k],\n",
    "                                                               classes=['Real','Fake'],num_workers=1)\n",
    "    dsn_test_loader, image_classes = DenseNetTestConstructor(path_to_test='F:/data/testset/cyclegan/'+ test_categories[k],\n",
    "                                                               classes=['Real','Fake'],num_workers=1)\n",
    "    fft_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/FFT/CNN_synth_testset/cyclegan/'\n",
    "                                                             + test_categories[k],classes=['Real','Fake'],num_workers=1)\n",
    "    dct_test_loader, image_classes = SpectrumTestConstructor(path_to_test='C:/Users/22710/Desktop/DCT/CNN_synth_testset/cyclegan/'\n",
    "                                                             + test_categories[k],classes=['Real','Fake'],num_workers=1)\n",
    "\n",
    "    car_model = []\n",
    "    test_loader_all = []\n",
    "    test_loader_all.append(xcp_test_loader)\n",
    "    test_loader_all.append(dsn_test_loader)\n",
    "    test_loader_all.append(fft_test_loader)\n",
    "    test_loader_all.append(dct_test_loader)\n",
    "    car_model.append(Xception_sofa)\n",
    "    car_model.append(DenseNet_sofa)\n",
    "    car_model.append(Spec_sofa)\n",
    "    car_model.append(ResNet34DCT_sofa)\n",
    "\n",
    "    weights_real = []\n",
    "    weights_fake = []\n",
    "    target_all = []\n",
    "    for j in range(4):\n",
    "        weight_real = []\n",
    "        weight_fake = []\n",
    "        target_single = []\n",
    "        model = car_model[j]\n",
    "        model.eval()\n",
    "        # specify loss function (categorical cross-entropy)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # specify optimizer\n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': list(model.parameters())[:-1], 'lr': 1e-3, 'momentum': 0.9, 'weight_decay': 1e-3},\n",
    "            {'params': list(model.parameters())[-1], 'lr': 5e-5, 'momentum': 0.9, 'weight_decay': 1e-5}\n",
    "        ])\n",
    "        # iterate over test data\n",
    "        for data, target in test_loader_all[j]:\n",
    "        #     print(target)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "        #     print(output)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # compare predictions to true label\n",
    "            probabilities = convert(output)\n",
    "            num_output = probabilities.cpu().detach().numpy()\n",
    "            num_target = target.cpu().detach().numpy()\n",
    "            for i in range(2):\n",
    "                weight_real.append(num_output[i][0])\n",
    "                weight_fake.append(num_output[i][1])\n",
    "                target_single.append(num_target[i])\n",
    "\n",
    "        weights_real.append(weight_real)\n",
    "        weights_fake.append(weight_fake)\n",
    "        target_all.append(target_single)\n",
    "    \n",
    "    count_real_correct = 0\n",
    "    count_fake_correct = 0\n",
    "    num_real = target_all[0].count(0)\n",
    "    num_fake = target_all[0].count(1)\n",
    "    for i in range(num_real+num_fake):\n",
    "        weighted_real = (weights_real[0][i] + weights_real[1][i] + weights_real[2][i] + weights_real[3][i])/4\n",
    "        weighted_fake = (weights_fake[0][i] + weights_fake[1][i] + weights_fake[2][i] + weights_fake[3][i])/4\n",
    "        if weighted_real > weighted_fake:\n",
    "            if i < num_real:\n",
    "                count_real_correct += 1\n",
    "        else:\n",
    "            if i >= num_real:\n",
    "                count_fake_correct += 1\n",
    "    \n",
    "    print('Test Accuracy of Real: %2d%% (%2d/%2d)'%(100*count_real_correct/num_real, count_real_correct, num_real))\n",
    "    print('Test Accuracy of Fake: %2d%% (%2d/%2d)'%(100*count_fake_correct/num_fake, count_fake_correct, num_fake))\n",
    "    print('Test Accuracy (Overall): %2d%% (%2d/%2d)'%(100*(count_real_correct+count_fake_correct)/(num_real+num_fake), \n",
    "                                                      count_real_correct+count_fake_correct, num_real+num_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bc38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
